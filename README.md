# projectEngine – Backend

This repository contains the infrastructure for running asynchronous “build jobs” for projectEngine. Authenticated users can create a job, follow its progress over WebSockets, and fetch the final generated app artifact once it completes.

## Stack

- Django + Django REST Framework for the HTTP API
- Channels + Daphne for WebSockets
- Celery workers for background execution
- Redis (channel layer + Celery broker/result backend)
- PostgreSQL for persistence
- JWT authentication provided by the existing `authentication` app

## Domain Model

| Model    | Purpose                                                                                  |
|----------|------------------------------------------------------------------------------------------|
| `Job`    | Tracks the lifecycle of a build (owner, prompt, status, timestamps, optional error).    |
| `JobStep`| Ordered log of intermediate updates produced by the multi-agent system.                 |
| `App`    | The final structured artifact (JSON) generated by the job.                              |

Jobs emit updates to the WebSocket group `job_<job_id>` and persist the same information in the database so clients can reconcile on reconnect.

## API Overview

- `POST /api/jobs/` – create a job for the authenticated user, returns the job id and initial state.
- `GET /api/jobs/<job_id>/` – fetch job details + ordered steps (owner-only).
- `GET /api/jobs/` – list the caller’s jobs (paginated).
- `GET /api/apps/` – list app artifacts owned by the caller.
- `GET /api/apps/<id>/` – retrieve an app by ID.
- `GET /api/apps/by-job/<job_id>/` – fetch the app that belongs to a specific job.
- WebSocket: `ws://<host>/ws/jobs/<job_id>/` – live updates for an owned job.

All endpoints require authentication (JWT) except those under `/api/auth/`.

## Background Execution Flow

1. `POST /api/jobs/` stores a `Job` with status `queued` and enqueues `jobs.tasks.run_job_task`.
2. The Celery worker loads the configured orchestrator (defaults to the dummy stub).
3. The orchestrator receives a `JobCallbacks` instance and uses it to:
   - emit status transitions,
   - append agent steps,
   - publish the final `App`.
4. Every callback invocation updates the database **and** broadcasts a WebSocket payload to `job_<job_id>`.

To integrate the real multi-agent system, expose a callable and set `AGENT_ORCHESTRATOR_PATH` (env var) to its dotted import path. See `jobs/agent_client.py` for expectations.

## Running Locally (Docker)

```bash
cd server
docker-compose up --build
```

Services:
- `web`: Django + Daphne ASGI server (HTTP + WebSockets)
- `worker`: Celery worker processing background jobs
- `db`: PostgreSQL 16
- `redis`: Broker/channel layer

The compose file automatically runs migrations on start. Copy `.envtemplate` to `.env` and adjust secrets before running in anything other than local development.

## Running Without Docker

1. Install dependencies:
   ```bash
   cd server
   python -m venv .venv && .venv\Scripts\activate  # Windows example
   pip install -r requirements.txt
   ```
2. Start Redis and Postgres locally.
3. Export the environment variables from `.envtemplate`.
4. Launch services:
   ```bash
   python manage.py migrate
   daphne -b 0.0.0.0 -p 8000 server.asgi:application  # HTTP + WS
   celery -A server worker -l info                    # background jobs
   ```

## Testing the WebSocket

1. Create a job via `POST /api/jobs/`.
2. Connect to `ws://localhost:8000/ws/jobs/<job_id>/` with the same JWT you used for HTTP (Channels shares Django auth; plug in your JWT-to-scope middleware as needed).
3. Watch for `status`, `step`, and `app` payloads as the job progresses.

## Next Steps

- Plug in the real multi-agent orchestration module using `AGENT_ORCHESTRATOR_PATH`.
- Harden authentication for Channels scope (e.g., JWT auth middleware).
- Add observability (metrics/log forwarding) for long-running jobs.